{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4da38ad",
   "metadata": {},
   "source": [
    "# 1. Libraries & Sample Data\n",
    "The first step is to load our Python Libraries and download the sample data. The dataset represents Apple stock price (1d bars) for the year 2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa05430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Python Libraries\n",
    "import math\n",
    "import keras\n",
    "import random\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# for dataframe display\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "def display_df(df):\n",
    "    # Puts the scrollbar next to the DataFrame\n",
    "    display(HTML(\"<div style='height: 200px; overflow: auto; width: fit-content'>\" + df.to_html() + \"</div>\"))\n",
    "\n",
    "# for reproducability of answers\n",
    "keras.utils.set_random_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa831031",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Sample Data GOOG_2009-2010_6m_normed_1d (cleaned & normalized, with features)\n",
    "data = pd.read_csv('GOOG_2009-2010_6m_normed_1d.csv')\n",
    "# track index to remember which feature is which\n",
    "idx_close = 0\n",
    "idx_bb_upper = 1\n",
    "idx_bb_lower = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c77a8",
   "metadata": {},
   "source": [
    "# 2. Define the Agent\n",
    "Now that our data is ready to use, we can define the Reinforcement Learning Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd06b6d",
   "metadata": {},
   "source": [
    "### Define the DQN Model\n",
    "The first step in defining our agent is the Deep Q-Network model definition. For this excercise, we are creating a sequential model with three layers. The first two layers have output shape of 32 and 8, respectively, and a RELU activation. The output layer has an output shape of the size of our action space (buy, sell, hold), and a linear activation. Our Loss function is Mean Squared Error, and our optimizer is Adam with a learning rate of 0.001. Use Keras to build this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e98591",
   "metadata": {},
   "outputs": [],
   "source": [
    "@keras.saving.register_keras_serializable()\n",
    "# Define DQN Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edabcd7d",
   "metadata": {},
   "source": [
    "### Define Agent Class\n",
    "Now that we have defined our underlying DQN Model, we must define out Reinforcement Learning Agent. The agent initialization is provided for you, you must define an act function, and an expereince replay function. As a reminder, the act function defines how our model will act (buy, hold, or sell) given a certain state. The Experience Replay function tackles catastrophic forgetting in our training process, by maintaining a memory buffer to allow training on independent / randomized minibatches of previous states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1489f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, window_size, num_features, test_mode=False, model_name=''):\n",
    "        \n",
    "\n",
    "\n",
    "    #Deep Q Learning (DQL) model\n",
    "    def _model(self):\n",
    "        \n",
    "    \n",
    "\n",
    "    # DQL Predict (with input reshaping)\n",
    "    #   Input = State\n",
    "    #   Output = Q-Table of action Q-Values\n",
    "    def get_q_values_for_state(self, state):\n",
    "    \n",
    "\n",
    "    # DQL Fit (with input reshaping)\n",
    "    #   Input = State, Target Q-Table \n",
    "    #   Output = MSE Loss between Target Q-Table and Actual Q-Table for State\n",
    "    def fit_model(self, input_state, target_output):\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project-udacity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
