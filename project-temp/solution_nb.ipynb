{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "784fdcbd",
   "metadata": {},
   "source": [
    "# 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969bd837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Env Setup:\n",
    "# conda create project-env python=3.11\n",
    "\n",
    "# pip install scikit-learn\n",
    "# pip install yfinance\n",
    "# pip install matplotlib\n",
    "# pip install seaborn\n",
    "# pip install tensorflow-cpu\n",
    "# pip install keras\n",
    "\n",
    "# python -m ipykernel install --user --name project-env --display-name \"Python (project-env)\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d2f0d6",
   "metadata": {},
   "source": [
    "### Check that kernel installed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb0ac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.executable\n",
    "# Desired output = '/home/<user>/anaconda3/envs/project-env/bin/python'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4713f093",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084c6888-0ae9-44a8-b7fa-8f1f03df3172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import yfinance as yf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import keras\n",
    "import datetime\n",
    "import math\n",
    "import random\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from IPython.core.debugger import set_trace\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3425f45",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d8ee33-5b3a-43de-beb8-5da7cab900fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Apple Ticker Data\n",
    "data = yf.download(\"AAPL\", start=\"2010-01-01\", end=\"2010-02-01\", interval=\"1d\")\n",
    "for i in range(4, len(data.index), int(len(data.index)/5)):\n",
    "    data.iloc[i] = np.nan\n",
    "data.to_csv('aapl_2010_2020_1d.csv')\n",
    "data = pd.read_csv('aapl_2010_2020_1d.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4a4a35",
   "metadata": {},
   "source": [
    "# 2. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aedbfa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.width', 100)\n",
    "display(data.head())\n",
    "display(data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b401f82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Close'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189bc851",
   "metadata": {},
   "source": [
    "# 3. Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ebfafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print('Number of Null Values =', data.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9128668",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward fill missing values\n",
    "data=data.ffill()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6050c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "print('Number of Null Values =', data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e8526a",
   "metadata": {},
   "source": [
    "# 4. Dataset Construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7557879",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=list(data[\"Close\"])\n",
    "X=[float(x) for x in X]\n",
    "validation_size = 0.2\n",
    "#In case the data is not dependent on the time series, then train and test split should be done based on sequential sample\n",
    "#This can be done by selecting an arbitrary split point in the ordered list of observations and creating two new datasets.\n",
    "train_size = int(len(X) * (1-validation_size))\n",
    "X_train, X_test = X[0:train_size], X[train_size:len(X)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714c77a8",
   "metadata": {},
   "source": [
    "# 5. Define the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd06b6d",
   "metadata": {},
   "source": [
    "### Define the DQN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e98591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "class DQN():\n",
    "    def __init__(self, state_size, action_size):\n",
    "    \n",
    "        model = keras.models.Sequential()\n",
    "        #Input Layer\n",
    "        model.add(keras.layers.Dense(units=64, input_dim=state_size, activation=\"relu\"))\n",
    "        #Hidden Layers\n",
    "        model.add(keras.layers.Dense(units=32, activation=\"relu\"))\n",
    "        model.add(keras.layers.Dense(units=8, activation=\"relu\"))\n",
    "        #Output Layer \n",
    "        model.add(keras.layers.Dense(action_size, activation=\"linear\"))\n",
    "        model.compile(loss=\"mse\", optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "        self.model = model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edabcd7d",
   "metadata": {},
   "source": [
    "### Define Agent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1489f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, state_size, is_eval=False, model_name=\"\"):\n",
    "        #State size depends and is equal to the the window size, n previous days\n",
    "        self.state_size = state_size # normalized previous days, \n",
    "        self.action_size = 3 # sit, buy, sell\n",
    "        self.memory = deque(maxlen=1000)\n",
    "        self.inventory = []\n",
    "        self.model_name = model_name\n",
    "        self.is_eval = is_eval\n",
    "\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.995\n",
    "        #self.epsilon_decay = 0.9\n",
    "        \n",
    "        #self.model = self._model()\n",
    "\n",
    "        self.model = keras.models.load_model(model_name) if is_eval else self._model()\n",
    "\n",
    "    #Deep Q Learning model- returns the q-value when given state as input \n",
    "    def _model(self):\n",
    "        model = DQN(self.state_size, self.action_size).model\n",
    "        return model\n",
    "    \n",
    "    #Return the action on the value function\n",
    "    #With probability (1-$\\epsilon$) choose the action which has the highest Q-value.\n",
    "    #With probability ($\\epsilon$) choose any action at random.\n",
    "    #Intitially high epsilon-more random, later less\n",
    "    #The trained agents were evaluated by different initial random condition\n",
    "    #and an e-greedy policy with epsilon 0.05. This procedure is adopted to minimize the possibility of overfitting during evaluation.\n",
    " \n",
    "    def act(self, state): \n",
    "        #If it is test and self.epsilon is still very high, once the epsilon become low, there are no random\n",
    "        #actions suggested.\n",
    "        if not self.is_eval and random.random() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)        \n",
    "        options = self.model.predict(state)\n",
    "        #set_trace()\n",
    "        #action is based on the action that has the highest value from the q-value function.\n",
    "        return np.argmax(options[0])\n",
    "\n",
    "    def expReplay(self, batch_size):\n",
    "        mini_batch = []\n",
    "        l = len(self.memory)\n",
    "        for i in range(l - batch_size + 1, l):\n",
    "            mini_batch.append(self.memory[i])\n",
    "        \n",
    "        # the memory during the training phase. \n",
    "        for state, action, reward, next_state, done in mini_batch:\n",
    "            target = reward # reward or Q at time t    \n",
    "            #update the Q table based on Q table equation\n",
    "            #set_trace()\n",
    "            if not done:\n",
    "                #set_trace()\n",
    "                #max of the array of the predicted. \n",
    "                target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])     \n",
    "                \n",
    "            # Q-value of the state currently from the table    \n",
    "            target_f = self.model.predict(state)\n",
    "            # Update the output Q table for the given action in the table     \n",
    "            target_f[0][action] = target\n",
    "            #train and fit the model where state is X and target_f is Y, where the target is updated. \n",
    "            self.model.fit(state, target_f, epochs=1, verbose=0)\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfee09a6",
   "metadata": {},
   "source": [
    "# 6. Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a05cd0f4",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "722fbbdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints formatted price\n",
    "def formatPrice(n):\n",
    "    return (\"-$\" if n < 0 else \"$\") + \"{0:.2f}\".format(abs(n))\n",
    "\n",
    "# returns the sigmoid\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# returns an an n-day state representation ending at time t\n",
    "\n",
    "def getState(dataset, t, n):    \n",
    "    d = t - n + 1\n",
    "    block = dataset[d:t + 1] if d >= 0 else -d * [dataset[0]] + dataset[0:t + 1] # pad with t0\n",
    "    #block is which is the for [1283.27002, 1283.27002]\n",
    "    res = []\n",
    "    for i in range(n - 1):\n",
    "        res.append(sigmoid(block[i + 1] - block[i]))\n",
    "    return np.array([res])\n",
    "\n",
    "# Plots the behavior of the output\n",
    "def plot_behavior(data_input, states_buy, states_sell, profit):\n",
    "    fig = plt.figure(figsize = (15,5))\n",
    "    plt.plot(data_input, color='r', lw=2.)\n",
    "    plt.plot(data_input, '^', markersize=10, color='m', label = 'Buying signal', markevery = states_buy)\n",
    "    plt.plot(data_input, 'v', markersize=10, color='k', label = 'Selling signal', markevery = states_sell)\n",
    "    plt.title('Total gains: %f'%(profit))\n",
    "    plt.legend()\n",
    "    #plt.savefig('output/'+name+'.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6bd8f6",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 1\n",
    "agent = Agent(window_size)\n",
    "#In this step we feed the closing value of the stock price \n",
    "data = X_train\n",
    "l = len(data) - 1\n",
    "#\n",
    "batch_size = 32\n",
    "#An episode represents a complete pass over the data.\n",
    "episode_count = 2\n",
    "\n",
    "for e in range(episode_count + 1):\n",
    "    print(\"Running episode \" + str(e) + \"/\" + str(episode_count))\n",
    "    state = getState(data, 0, window_size + 1)\n",
    "    #set_trace()\n",
    "    total_profit = 0\n",
    "    agent.inventory = []\n",
    "    states_sell = []\n",
    "    states_buy = []\n",
    "    for t in range(l):\n",
    "        action = agent.act(state)    \n",
    "        # sit\n",
    "        next_state = getState(data, t + 1, window_size + 1)\n",
    "        reward = 0\n",
    "\n",
    "        if action == 1: # buy\n",
    "            agent.inventory.append(data[t])\n",
    "            states_buy.append(t)\n",
    "            #print(\"Buy: \" + formatPrice(data[t]))\n",
    "\n",
    "        elif action == 2 and len(agent.inventory) > 0: # sell\n",
    "            bought_price = agent.inventory.pop(0)      \n",
    "            reward = max(data[t] - bought_price, 0)\n",
    "            total_profit += data[t] - bought_price\n",
    "            states_sell.append(t)\n",
    "            #print(\"Sell: \" + formatPrice(data[t]) + \" | Profit: \" + formatPrice(data[t] - bought_price))\n",
    "\n",
    "        done = True if t == l - 1 else False\n",
    "        #appends the details of the state action etc in the memory, which is used further by the exeReply function\n",
    "        agent.memory.append((state, action, reward, next_state, done))\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            print(\"--------------------------------\")\n",
    "            print(\"Total Profit: \" + formatPrice(total_profit))\n",
    "            print(\"--------------------------------\")\n",
    "            #set_trace()\n",
    "            #pd.DataFrame(np.array(agent.memory)).to_csv(\"Agent\"+str(e)+\".csv\")\n",
    "            #Chart to show how the model performs with the stock goin up and down for each \n",
    "            plot_behavior(data,states_buy, states_sell, total_profit)\n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.expReplay(batch_size)    \n",
    "            \n",
    "\n",
    "    if e % 2 == 0:\n",
    "        agent.model.save(\"model_ep\" + str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b3f476",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl-project-udacity",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
